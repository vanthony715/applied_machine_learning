{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59608276",
   "metadata": {},
   "source": [
    "## Describe the environment in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a98c10",
   "metadata": {},
   "source": [
    "The environment of Nim is the board itself, though it should be noted that the board can be a \n",
    "\n",
    "tabletop, a book, or anything flat that can adequately support the items used to play Nim.\n",
    "\n",
    "Ref: Graham, Paul, Lord William, Reinforcement Learning and the Game of Nim, Stockholm Sweden, 2015.\n",
    "accessed: http://www.diva-portal.org/smash/get/diva2:814832/FULLTEXT01.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b764fd7",
   "metadata": {},
   "source": [
    "## Describe the agent(s) in the Nim learning model (Hint, not just the Q-learner)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa9c50",
   "metadata": {},
   "source": [
    "1. Random player\n",
    "Random player is an agent that moves stochastically and does not learn from the environment.\n",
    "\n",
    "\n",
    "2. Guru player\n",
    "\n",
    "Guru player is an agent that moves according to a known solution to the Nim game and does not\n",
    "\n",
    "need to learn.\n",
    "\n",
    "\n",
    "3. Q-learner\n",
    "\n",
    "The Q-learner player is an agent that moves according to a learned Q-table generated by the \n",
    "\n",
    "interaction of the player with the envionment, i.e. state and action pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3088fc",
   "metadata": {},
   "source": [
    "## Describe the reward and penalty in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8648aa",
   "metadata": {},
   "source": [
    "The end state for the winning agent to be in is the state where the agent takes the last items of \n",
    "\n",
    "the last pile, which I will call the best state. The Q-table is updated with the reward for each \n",
    "\n",
    "action that the agent makes for this version of Nim. If the agent arrives in \n",
    "\n",
    "the best state, then the Q-table the agent will be rewarded with 100.0, otherwise the reward is\n",
    "\n",
    "zero, and the Q-table is updated accordingly. I do not see an explicit penalty for not arriving in \n",
    "\n",
    "the best state, unless the penalty is getting zero reward, which I would argue is not a penalty, \n",
    "\n",
    "since a reward is not deducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc257ff",
   "metadata": {},
   "source": [
    "## How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3a9db",
   "metadata": {},
   "source": [
    "According to the game rules, a player can take any number of items from one to ten from one of any \n",
    "\n",
    "of the piles at a time. In addition, the player can choose to not take any of the items for one turn\n",
    "\n",
    "per pile. Translating to reinforcement learning; there are eleven states for each pile, because the \n",
    "\n",
    "the size of the Nim state space for each pile is number of items in a given pile (item_num) + the \n",
    "\n",
    "one time chance to not take any pieces (1). An equation that can be used to determine the  number of \n",
    "\n",
    "states in this cas is num_states_per_pile = item_num + 1. The equation for all of the states is the \n",
    "\n",
    "num_states_per_pile to the power of the number of piles (pile_num), which can be written as \n",
    "\n",
    "all_possible_states_num = (num_states_per_pile) ^ pile_num = (10 + 1) ^ 3 = 11 ^ 3 = 1331. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63b53c",
   "metadata": {},
   "source": [
    "## How many possible unique actions there could be for player 1 in the Nim game with 10 items per pile and 3 piles total?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586f5a1",
   "metadata": {},
   "source": [
    "The player can take any number of items from the pile to include taking no items. Because the player \n",
    "\n",
    "can only choose to take 0 items per pile once, it is tempting to add the state of not taking an \n",
    "\n",
    "item, but it seems that this line of thinking is false, since the state prior to action is equal to \n",
    "\n",
    "the post-action state, therefore, it should not be added as a unique action.\n",
    "\n",
    "Representing this mathematically, and borrowing the variable developed in problem 4, we arrive at \n",
    "\n",
    "num_states_player_1 = num_states_per_pile1 + num_states_per_pile2 + num_state_per_pile3 = 10 + 10 + 10 = 30."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c60dd2",
   "metadata": {},
   "source": [
    "## Find a way to improve the provided Nim game learning model. Do you think onecan beat the Guru player? (Hint: How about penalizing the losses? Hint: It is indeed possible to find a better solution, which improves the way Q-learning updates its Q-table).\n",
    "\n",
    "# Note: Do not use a Guru player inside the learning module as this would defeat the purpose of reinforcement learning (why?). However, playing with a Guru is permitted.\n",
    "\n",
    "The reason that putting a guru player inside the learning module is that the guru acts according to the known solution, whereas the Q-Learner does not know the solution, but learns from the environment given a set of rules. \n",
    "\n",
    "I hypothesize the following:\n",
    "\n",
    "1. Training the Q-learner more then the 1000 iterations that it was set to train by default will\n",
    "\n",
    "increase the Q-learners performance.\n",
    "\n",
    "\n",
    "__Test 1__: Train the learner with the default 1000 iterations and have Q-learner play against the \n",
    "\n",
    "random and guru agents for 1000 games, then repeat with 10,000 training iterations and compare \n",
    "\n",
    "results.\n",
    "\n",
    "\n",
    "2. Penalizing each move that the Q-learner makes will improve the Q-learner's performance. This \n",
    "\n",
    "hypothesis might only be true for a range of penalty values. \n",
    "\n",
    "\n",
    "__Test 2__: Set a negative penalty value (modify code in nim_qlearn)for each move that the Q-leaner \n",
    "\n",
    "makes that lands the agent in a non-winning state. This can be any value that shows a positive \n",
    "\n",
    "increase in the performance of the Q-learner.\n",
    "\n",
    "\n",
    "3. The Q-learner will perform better by using the methods of hypothesis 1 and hypothesis 2 combined. \n",
    "\n",
    "\n",
    "__Test 3__: Train the Q-learner play at 10,000 training iterations with a negative penalty for each \n",
    "\n",
    "action that the learner takes that lands it in a non-winning state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab55772",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note this code is taken from the module notebook: module09_reinforcement_notebook.html with \n",
    "minor modifications.\n",
    "\n",
    "Initialize the game and define the agents.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint, choice\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move\n",
    "\n",
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action\n",
    "\n",
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca14b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterate through user-defined number of games and update the Q-learners Q-table\n",
    "\n",
    "Note this code is taken from the module notebook: module09_reinforcement_notebook.html with \n",
    "minor modifications.\n",
    "'''\n",
    "\n",
    "qtable, Alpha, Gamma, Reward = None, 0.9, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n, penalty):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            qtable_update(penalty, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f26e7",
   "metadata": {},
   "source": [
    "## Baseline: The Qlearner learns from the default 1000 iterations of training, and all learners play 1000 games. There are no penalties for losing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8971e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Game Results\n",
      "---------------------------------\n",
      "\n",
      "Agent1:  Random Score:  510\n",
      "Agent2:  Random score:  490\n",
      "\n",
      "Agent1:  Random Score:  9\n",
      "Agent2:  Guru score:  991\n",
      "\n",
      "Agent1:  Random Score:  316\n",
      "Agent2:  Qlearner score:  684\n",
      "\n",
      "Agent1:  Guru Score:  1000\n",
      "Agent2:  Random score:  0\n",
      "\n",
      "Agent1:  Guru Score:  935\n",
      "Agent2:  Guru score:  65\n",
      "\n",
      "Agent1:  Guru Score:  999\n",
      "Agent2:  Qlearner score:  1\n",
      "\n",
      "Agent1:  Qlearner Score:  692\n",
      "Agent2:  Random score:  308\n",
      "\n",
      "Agent1:  Qlearner Score:  14\n",
      "Agent2:  Guru score:  986\n",
      "\n",
      "Agent1:  Qlearner Score:  552\n",
      "Agent2:  Qlearner score:  448\n",
      "\n",
      "\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##teach the q-learner by playing the game \n",
    "learn_num_games = 1000\n",
    "play_num_games = 1000\n",
    "explore_penalty = 0\n",
    "nim_qlearn(learn_num_games, explore_penalty)\n",
    "\n",
    "##list of different agents\n",
    "agents = ('Random', 'Guru', 'Qlearner')\n",
    "\n",
    "##keep track of games\n",
    "results = {'agent': [], 'score': [], 'turn': [], 'descr': []}\n",
    "\n",
    "##iterate through agents for the first player to go\n",
    "print('\\nBaseline Game Results')\n",
    "print('---------------------------------')\n",
    "for agent1 in agents:\n",
    "    ##iterate through agents for the second player to go\n",
    "    for agent2 in agents:\n",
    "        game_agents = agent1 + '_vs_' + agent2\n",
    "        a1_score, a2_score = play_games(play_num_games, agent1, agent2)\n",
    "        \n",
    "        ##record agent1 performance\n",
    "        results['agent'].append(agent1)\n",
    "        results['score'].append(a1_score)\n",
    "        results['turn'].append('first')\n",
    "        results['descr'].append('baseline')\n",
    "        \n",
    "        ##record agent2 performance\n",
    "        results['agent'].append(agent2)\n",
    "        results['score'].append(a2_score)\n",
    "        results['turn'].append('second')\n",
    "        results['descr'].append('baseline')\n",
    "        \n",
    "        print('\\nAgent1: ',agent1, 'Score: ', a1_score)\n",
    "        print('Agent2: ', agent2, 'score: ', a2_score)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7853dcd",
   "metadata": {},
   "source": [
    "## Qlearner train for 10,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5bb6a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qlearner 10,000 Train Iters Game Results\n",
      "---------------------------------\n",
      "\n",
      "Agent1:  Random Score:  480\n",
      "Agent2:  Random score:  520\n",
      "\n",
      "Agent1:  Random Score:  9\n",
      "Agent2:  Guru score:  991\n",
      "\n",
      "Agent1:  Random Score:  258\n",
      "Agent2:  Qlearner score:  742\n",
      "\n",
      "Agent1:  Guru Score:  999\n",
      "Agent2:  Random score:  1\n",
      "\n",
      "Agent1:  Guru Score:  934\n",
      "Agent2:  Guru score:  66\n",
      "\n",
      "Agent1:  Guru Score:  994\n",
      "Agent2:  Qlearner score:  6\n",
      "\n",
      "Agent1:  Qlearner Score:  739\n",
      "Agent2:  Random score:  261\n",
      "\n",
      "Agent1:  Qlearner Score:  29\n",
      "Agent2:  Guru score:  971\n",
      "\n",
      "Agent1:  Qlearner Score:  929\n",
      "Agent2:  Qlearner score:  71\n",
      "\n",
      "\n",
      "CPU times: total: 438 ms\n",
      "Wall time: 432 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##teach the q-learner by playing the game \n",
    "learn_num_games = 10000\n",
    "play_num_games = 1000\n",
    "explore_penalty = 0\n",
    "nim_qlearn(learn_num_games, explore_penalty)\n",
    "\n",
    "##iterate through agents for the first player to go\n",
    "print('\\nQlearner 10,000 Train Iters Game Results')\n",
    "print('---------------------------------')\n",
    "for agent1 in agents:\n",
    "    ##iterate through agents for the second player to go\n",
    "    for agent2 in agents:\n",
    "        a1_score, a2_score = play_games(play_num_games, agent1, agent2)\n",
    "        \n",
    "        ##record agent1 performance\n",
    "        results['agent'].append(agent1)\n",
    "        results['score'].append(a1_score)\n",
    "        results['turn'].append('first')\n",
    "        results['descr'].append('base_increased_train_it')\n",
    "        \n",
    "        ##record agent2 performance\n",
    "        results['agent'].append(agent2)\n",
    "        results['score'].append(a2_score)\n",
    "        results['turn'].append('second')\n",
    "        results['descr'].append('base_increased_train_it')\n",
    "        \n",
    "        print('\\nAgent1: ',agent1, 'Score: ', a1_score)\n",
    "        print('Agent2: ', agent2, 'score: ', a2_score)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab9d8d",
   "metadata": {},
   "source": [
    "## Q-learner is penalized for actions that do not result in a win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494fb2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qlearner Explore Penalty Game Results\n",
      "---------------------------------\n",
      "\n",
      "Agent1:  Random Score:  512\n",
      "Agent2:  Random score:  488\n",
      "\n",
      "Agent1:  Random Score:  11\n",
      "Agent2:  Guru score:  989\n",
      "\n",
      "Agent1:  Random Score:  273\n",
      "Agent2:  Qlearner score:  727\n",
      "\n",
      "Agent1:  Guru Score:  997\n",
      "Agent2:  Random score:  3\n",
      "\n",
      "Agent1:  Guru Score:  942\n",
      "Agent2:  Guru score:  58\n",
      "\n",
      "Agent1:  Guru Score:  999\n",
      "Agent2:  Qlearner score:  1\n",
      "\n",
      "Agent1:  Qlearner Score:  724\n",
      "Agent2:  Random score:  276\n",
      "\n",
      "Agent1:  Qlearner Score:  12\n",
      "Agent2:  Guru score:  988\n",
      "\n",
      "Agent1:  Qlearner Score:  542\n",
      "Agent2:  Qlearner score:  458\n",
      "\n",
      "\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##teach the q-learner by playing the game \n",
    "learn_num_games = 1000\n",
    "play_num_games = 1000\n",
    "explore_penalty = -15.0\n",
    "nim_qlearn(learn_num_games, explore_penalty)\n",
    "\n",
    "##iterate through agents for the first player to go\n",
    "print('\\nQlearner Explore Penalty Game Results')\n",
    "print('---------------------------------')\n",
    "for agent1 in agents:\n",
    "    ##iterate through agents for the second player to go\n",
    "    for agent2 in agents:\n",
    "        a1_score, a2_score = play_games(play_num_games, agent1, agent2)\n",
    "        \n",
    "        ##record agent1 performance\n",
    "        results['agent'].append(agent1)\n",
    "        results['score'].append(a1_score)\n",
    "        results['turn'].append('first')\n",
    "        results['descr'].append('qlearner_with_penalty')\n",
    "        \n",
    "        ##record agent2 performance\n",
    "        results['agent'].append(agent2)\n",
    "        results['score'].append(a2_score)\n",
    "        results['turn'].append('second')\n",
    "        results['descr'].append('qlearner_with_penalty')\n",
    "        \n",
    "        print('\\nAgent1: ',agent1, 'Score: ', a1_score)\n",
    "        print('Agent2: ', agent2, 'score: ', a2_score)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a7ece",
   "metadata": {},
   "source": [
    "## Q-learner trained for 100,000 training iterations and penalized for actions that do not result in a win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f513ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qlearner with 10000 Training iterations and Explore Penalty Game Results\n",
      "---------------------------------\n",
      "\n",
      "Agent1:  Random Score:  516\n",
      "Agent2:  Random score:  484\n",
      "\n",
      "Agent1:  Random Score:  6\n",
      "Agent2:  Guru score:  994\n",
      "\n",
      "Agent1:  Random Score:  299\n",
      "Agent2:  Qlearner score:  701\n",
      "\n",
      "Agent1:  Guru Score:  1000\n",
      "Agent2:  Random score:  0\n",
      "\n",
      "Agent1:  Guru Score:  949\n",
      "Agent2:  Guru score:  51\n",
      "\n",
      "Agent1:  Guru Score:  999\n",
      "Agent2:  Qlearner score:  1\n",
      "\n",
      "Agent1:  Qlearner Score:  734\n",
      "Agent2:  Random score:  266\n",
      "\n",
      "Agent1:  Qlearner Score:  26\n",
      "Agent2:  Guru score:  974\n",
      "\n",
      "Agent1:  Qlearner Score:  925\n",
      "Agent2:  Qlearner score:  75\n",
      "\n",
      "\n",
      "CPU times: total: 453 ms\n",
      "Wall time: 441 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##teach the q-learner by playing the game \n",
    "learn_num_games = 10000\n",
    "play_num_games = 1000\n",
    "explore_penalty = -15\n",
    "nim_qlearn(learn_num_games, explore_penalty)\n",
    "\n",
    "##iterate through agents for the first player to go\n",
    "print('\\nQlearner with 10000 Training iterations and Explore Penalty Game Results')\n",
    "print('---------------------------------')\n",
    "for agent1 in agents:\n",
    "    ##iterate through agents for the second player to go\n",
    "    for agent2 in agents:\n",
    "        a1_score, a2_score = play_games(play_num_games, agent1, agent2)\n",
    "        \n",
    "        ##record agent1 performance\n",
    "        results['agent'].append(agent1)\n",
    "        results['score'].append(a1_score)\n",
    "        results['turn'].append('first')\n",
    "        results['descr'].append('qlearner_penalty_increase_it')\n",
    "        \n",
    "        ##record agent2 performance\n",
    "        results['agent'].append(agent2)\n",
    "        results['score'].append(a2_score)\n",
    "        results['turn'].append('second')\n",
    "        results['descr'].append('qlearner_penalty_increase_it')\n",
    "        \n",
    "        print('\\nAgent1: ',agent1, 'Score: ', a1_score)\n",
    "        print('Agent2: ', agent2, 'score: ', a2_score)\n",
    "print('\\n')\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481f724",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e9e6f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for agent that went first\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>579.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>384.456012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>269.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>622.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>943.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score\n",
       "count    36.000000\n",
       "mean    579.555556\n",
       "std     384.456012\n",
       "min       6.000000\n",
       "25%     269.250000\n",
       "50%     622.000000\n",
       "75%     943.750000\n",
       "max    1000.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nStatistics for agent that went first')\n",
    "df[df['turn'] == 'first'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ef81eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for agent that went second\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>420.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>384.456012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>56.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>378.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>730.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>994.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "count   36.000000\n",
       "mean   420.444444\n",
       "std    384.456012\n",
       "min      0.000000\n",
       "25%     56.250000\n",
       "50%    378.000000\n",
       "75%    730.750000\n",
       "max    994.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nStatistics for agent that went second')\n",
    "df[df['turn'] == 'second'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57be05c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qlearner Baseline Performance\n",
      " score    398.5\n",
      "dtype: float64\n",
      "\n",
      "Qlearner Trained on 1,000,000 Iterations Performance\n",
      " score    419.333333\n",
      "dtype: float64\n",
      "\n",
      "Qlearner Exploration Penalty\n",
      " score    410.666667\n",
      "dtype: float64\n",
      "\n",
      "Qlearner Exploration Penalty and Increased Training Iterations\n",
      " score    410.333333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('\\nQlearner Baseline Performance\\n', df[(df.agent == 'Qlearner') & (df.descr == 'baseline')].mean())\n",
    "\n",
    "print('\\nQlearner Trained on 1,000,000 Iterations Performance\\n', \n",
    "      df[(df.agent == 'Qlearner') & (df.descr == 'base_increased_train_it')].mean())\n",
    "\n",
    "print('\\nQlearner Exploration Penalty\\n', \n",
    "      df[(df.agent == 'Qlearner') & (df.descr == 'qlearner_with_penalty')].mean())\n",
    "\n",
    "print('\\nQlearner Exploration Penalty and Increased Training Iterations\\n', \n",
    "      df[(df.agent == 'Qlearner') & (df.descr == 'qlearner_penalty_increase_it')].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b6a35",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Note: Here score is defined as the number of games won by an agent, which is based on a 1000 game \n",
    "\n",
    "series.\n",
    "\n",
    "- In general, agent's that went first in the game of Nim, scored higher about 28% higher than the \n",
    "\n",
    "agent that went second.\n",
    "\n",
    "\n",
    "- The highest score for the agent that went first was 1000, which is a perfect score.\n",
    "\n",
    "\n",
    "- The highest score for the agent that went second was 998, which is near perfect.\n",
    "\n",
    "\n",
    "- The lowest score for the agent that went first was 2.0.\n",
    "\n",
    "\n",
    "- The lowest score for the agent that went second was 0.\n",
    "\n",
    "\n",
    "- The Qlearner's mean-average score improved by about 5.0% by increasing the training iterations to\n",
    "\n",
    "  10,000 instead of 1000.\n",
    "  \n",
    " \n",
    " - The Qlearner's mean-average score improved by about 3.0% by introducing a exploration penalty of    \n",
    "   negative 15.0 for the agent's actions that do not result in a win.\n",
    "   \n",
    " \n",
    " - The Qlearner's mean-average score improved by about 3.0% by introducing an exploration penalty of    \n",
    "   negative 15.0 for the agent's actions that do not result in a win and also increasing the agent's    \n",
    "   training iterations to 10k.\n",
    "   \n",
    "   \n",
    "   \n",
    "   Summary:\n",
    "\n",
    "Of the three hypothesis made at the beginning of this assignment, only the first two were correct. \n",
    "\n",
    "First, increasing the training iterations helped the Qlearner increase performance by increasing the \n",
    "\n",
    "number of games won. Second, adding a penalty of negative 15.0 for when the agent took an action \n",
    "\n",
    "that did not result in a win. Though the third case of adding the penalty and increasing training \n",
    "\n",
    "iterations outperformed the baseline case, it did not do better then increasing train iterations \n",
    "\n",
    "alone or penalizing the agent for exploring the environment alone.\n",
    "\n",
    "\n",
    "The learning rate (alpha) and discount factor (gamma) were also experimented with to a small extent \n",
    "\n",
    "to see if either hyperparameter would make a significant difference. The values settled on were \n",
    "\n",
    "alpha and gamma of 0.9 and 0.8 respectively. Something that might also help performance would be an \n",
    "\n",
    "optimal value search for these two hyperparameters. This assignment has been much different then the \n",
    "\n",
    "previous assignments, or atleast the concepts are much different then supervised vs unsupervised \n",
    "\n",
    "learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
