{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4540da6a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ff4c9",
   "metadata": {},
   "source": [
    "### Using real datasets (can also be hypothetically constructed by yourself) define thefollowing feature types, and give example values from your dataset. How would you represent these features in a computer program? Numerical, Nominal, Date, Text, Image, Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23104b8",
   "metadata": {},
   "source": [
    "__Numerical__:\n",
    "- The Abalone dataset uses has an attribute called \"Rings\", which are the number of rings found on abalone shells. This attribute is numerical and could be represented as a floating point variable.\n",
    "\n",
    "__Nominal__:\n",
    "- The Abalone dataset has an attribute that is nominal called \"Sex\", which is either Male, Female, or Infant. These attributes can be one-hot-encoded and represented as integers.\n",
    "\n",
    "__Date__:\n",
    "- The Investing for Bitcoin for Oil dataset has an attribute called \"date\" in the form of year-month-date. The date attribute can be separated into three attributes: day, month, and year and can be represented as an integer. The three resulting features could then be standardized and normalized along with the rest of the data in that datset.\n",
    "\n",
    "__Text__:\n",
    "- Text data is similiar to nominal and so using the same example from above, the Abalone dataset has an attribute that is nominal called \"Sex\", which is either Male, Female, or Infant. These attributes can be one-hot-encoded and represented as integers.\n",
    "\n",
    "__Image__:\n",
    "- The mnist handwritten digits dataset has pixel values as features, as is the case with all images. Each pixel location may be represented as 32 bit integers, but after standardization and normalization will end up as a floating point variable.\n",
    "\n",
    "__Dependant Variable__:\n",
    "- The Iris dataset has a \"target\" column which can be one of three classes, setosa, virginica, and versicolor. These values can either be one-hot-encoded or represented as an integer (1, 2, 3) prior to training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633da6cf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62516a",
   "metadata": {},
   "source": [
    "Using online resources, research and find other classifier performance metrics\n",
    "which are also as common as the accuracy metric. Write down the mathematical equations and the meaning of the metrics that you found.\n",
    "\n",
    "__Acronyms__:\n",
    "- True Positive (TP)\n",
    "- False Positive (FP)\n",
    "- False Negative (FN)\n",
    "\n",
    "__metric__:\n",
    "\n",
    "- Precision\n",
    "\n",
    "__equation:__ \n",
    "\n",
    "- precision = TP / (TP + FP) \n",
    "\n",
    "__meaning__:\n",
    "\n",
    "- Precision is a common metric used in object detection and classification problems and is the percent of true positive classifications, or put another way, it is the percent of correct classifications.\n",
    "\n",
    "\n",
    "__metric__:\n",
    "\n",
    "- Recall\n",
    "\n",
    "__equation__: \n",
    "\n",
    "- Recall = TP / (TP + FN)\n",
    "\n",
    "__meaning__:\n",
    "\n",
    "- Recall is also a common metric in object detection and classification and represents a ratio of missed detections/classifications.\n",
    "\n",
    "\n",
    "__metric__:\n",
    "\n",
    "- F1 Score\n",
    "\n",
    "__equation__:\n",
    "\n",
    "- F1 = 2[(precision x recall) / (precision + recall)]\n",
    "\n",
    "__meaning__:\n",
    "\n",
    "- Recall and precision both how well certain aspects of a model performed, but a more comprehensive score is the F1-Score. It combines both precision and recall into one ratio. This is an especially good metric for use-cases that are interested in both high precision and recall scores.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cafecf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d5ff5",
   "metadata": {},
   "source": [
    "### Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file (not provided, you have to download it by yourself by following the instructions in the module Jupyter notebook). Display the correlation matrix where each row and column are the features, which should be an 8 by 8 matrix (should we use 'Serial no'?). You can use pandas DataFrame.corr() to verify correctness of yours. Remember, you are not allowed to used numpy methods mean(), and stdev() or other libraries for mean or standard deviation.Observe that the diagonal of this matrix should have all 1's and explain why? Since the last column can be used as the target (dependent) variable, what do you think about the correlations between all the variables? Which variable should be the most important for prediction of 'Chance of Admit'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb372a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca47a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "##get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "##get data path and open as a pandas dataframe\n",
    "data_path = cwd + '\\\\data\\\\Admission_Predict.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d629677",
   "metadata": {},
   "outputs": [],
   "source": [
    "##take a look at the data\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcPearsonCorrelation:\n",
    "    \n",
    "    \"\"\"Calculates the Pearson Correlation Matrix\"\"\"\n",
    "    \n",
    "    def __init__(self, target_col_name: str) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the CalcPearsonCorrelation Object\n",
    "        \n",
    "        :param target_col_name: Name of the dependent variable \n",
    "        \"\"\"\n",
    "        \n",
    "        self.target_col_name = target_col_name  \n",
    "    \n",
    "    def calcColumnMean(self, sample_column: list) -> float:\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates the mean of of each column of a dataset\n",
    "        \n",
    "        :param sample_column: Independent variable as a list\n",
    "        \"\"\"\n",
    "        \n",
    "        ##calculate column mean\n",
    "        col_sum = sum(sample_column)\n",
    "        col_len = len(sample_column)\n",
    "        \n",
    "        return round((col_sum / col_len), 2)\n",
    "        \n",
    "    def calcCovariance(self, col_vals: np.array, col_mean: float, targ_vals: np.array, target_mean: float) -> float:\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates covariance between a column and the target column\n",
    "        \n",
    "        :param col_vals: Array of a single column of dataset\n",
    "        :param col_mean: Mean value of column of dataset\n",
    "        :param target_vals: Array of target_vals\n",
    "        :param target_mean: Mean value of target column of dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        sum_vals = ((col_vals - col_mean) * (targ_vals - targ_mean)).sum()\n",
    "        \n",
    "        return sum_vals\n",
    "    \n",
    "    def calcStd(self, col_vals: np.array, col_mean: float) -> float:\n",
    "        \"\"\" \n",
    "        Calculates attribute standard deviation\n",
    "        \n",
    "        :param col_vals: Array of column values\n",
    "        :param col_mean: Mean of column values\n",
    "        \"\"\"\n",
    "        \n",
    "#         std = (((col_vals - col_mean) ** 2) / 10).sum()\n",
    "        \n",
    "        std = ((col_vals - col_mean) ** 2).sum()\n",
    "        \n",
    "        return std\n",
    "    \n",
    "    def calcPearsonCoef(self, xy_cov: float, x_std: float, y_std: float) -> float:\n",
    "        \n",
    "        \"\"\" \n",
    "        Calculates the Pearson Coefiscients of a dataset\n",
    "        \n",
    "        :param xy_cov: Covariance of x and y values\n",
    "        :param x_std: Standard deviation of x\n",
    "        :param y_std: standard deviation of y\n",
    "        \"\"\"\n",
    "        \n",
    "        coef = xy_cov / (x_std * y_std)\n",
    "        \n",
    "        return coef   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'Chance of Admit '\n",
    "\n",
    "##insantiate correlation object\n",
    "calc_obj = CalcPearsonCorrelation(target_name)\n",
    "\n",
    "##calculate target mean value and difference\n",
    "targ_mean = calc_obj.calcColumnMean(list(df[target_name]))\n",
    "targ_vals = np.array(df[target_name])\n",
    "targ_std = calc_obj.calcStd(targ_vals, targ_mean)\n",
    "\n",
    "##correlation numbers array of shape of dataset\n",
    "array_size = len(list(df)) - 1\n",
    "coef_array = np.zeros((array_size, array_size))\n",
    "# coef_dict = {'attrib': [], 'coef': []}\n",
    "\n",
    "##iterate through all dataset attributes to calculate the column mean\n",
    "attribs = list(df)\n",
    "for i in range(array_size):\n",
    "    \n",
    "    attrib = attribs[i]\n",
    "#     if attrib != target_name:\n",
    "    \n",
    "    for j in range(array_size):\n",
    "\n",
    "        col_mean = calc_obj.calcColumnMean(list(df[attrib]))\n",
    "        col_vals = np.array(df[attrib])\n",
    "#             print(col_vals.size)\n",
    "        xy_cov = calc_obj.calcCovariance(col_vals, col_mean, targ_vals, targ_mean)\n",
    "        col_std = calc_obj.calcStd(col_vals, col_mean)\n",
    "\n",
    "        ##calculate the Pearson Correlation score and save to dict\n",
    "        coef = calc_obj.calcPearsonCoef(xy_cov, targ_std, col_std)\n",
    "#             print(coef)\n",
    "        coef_array[i][j] = round(coef, 5)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
