{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0665258e",
   "metadata": {},
   "source": [
    "## Use your previous pre-processed dataset, keep the variables as one-hot encoded and develop a multiple linear regression model. Use your model to predict the target variable for the people with age 20, male, and generation X. What is the MAE error of this prediction? How many regression coefficients are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49f7500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head:\n",
      "\n",
      "     country  year     sex          age  suicides_no  population  \\\n",
      "0  Albania  1987    male  15-24 years           21      312900   \n",
      "1  Albania  1987    male  35-54 years           16      308000   \n",
      "2  Albania  1987  female  15-24 years           14      289700   \n",
      "3  Albania  1987    male    75+ years            1       21800   \n",
      "4  Albania  1987    male  25-34 years            9      274300   \n",
      "\n",
      "   suicides_100k_pop  gdp_per_capita_dollars       generation  \n",
      "0               6.71                     796     Generation X  \n",
      "1               5.19                     796           Silent  \n",
      "2               4.83                     796     Generation X  \n",
      "3               4.59                     796  G.I. Generation  \n",
      "4               3.28                     796          Boomers  \n",
      "\n",
      "\n",
      "Datatypes:\n",
      "\n",
      "  country                    object\n",
      "year                        int64\n",
      "sex                        object\n",
      "age                        object\n",
      "suicides_no                 int64\n",
      "population                  int64\n",
      "suicides_100k_pop         float64\n",
      "gdp_per_capita_dollars      int64\n",
      "generation                 object\n",
      "dtype: object\n",
      "\n",
      "Data Description: \n",
      "\n",
      "                year   suicides_no    population  suicides_100k_pop  \\\n",
      "count  27820.000000  27820.000000  2.782000e+04       27820.000000   \n",
      "mean    2001.258375    242.574407  1.844794e+06          12.816097   \n",
      "std        8.469055    902.047917  3.911779e+06          18.961511   \n",
      "min     1985.000000      0.000000  2.780000e+02           0.000000   \n",
      "25%     1995.000000      3.000000  9.749850e+04           0.920000   \n",
      "50%     2002.000000     25.000000  4.301500e+05           5.990000   \n",
      "75%     2008.000000    131.000000  1.486143e+06          16.620000   \n",
      "max     2016.000000  22338.000000  4.380521e+07         224.970000   \n",
      "\n",
      "       gdp_per_capita_dollars  \n",
      "count            27820.000000  \n",
      "mean             16866.464414  \n",
      "std              18887.576472  \n",
      "min                251.000000  \n",
      "25%               3447.000000  \n",
      "50%               9372.000000  \n",
      "75%              24874.000000  \n",
      "max             126352.000000  \n",
      "Wall time: 1.37 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:41: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Data exploration was not done to a great extent for this assignment, since this was a dataset \n",
    "previously explored and processed from module three.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "##get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "##get data path and open as a pandas dataframe\n",
    "datapath = cwd + '\\\\data\\\\master.csv'\n",
    "\n",
    "##get dataframe\n",
    "data = pd.read_csv(datapath)\n",
    "\n",
    "##check contents, dtypes, and description:\n",
    "print('\\nHead:\\n\\n ', data.head())\n",
    "print('\\n\\nDatatypes:\\n\\n ', data.dtypes)\n",
    "print('\\nData Description: \\n\\n', data.describe())\n",
    "\n",
    "# ##get feature names\n",
    "features = list(data)\n",
    "\n",
    "##picked a specific year to narrow down the data\n",
    "data = data[data.year == 2000]\n",
    "data = data[data.sex == 'male']\n",
    "\n",
    "##make copy for later analysis\n",
    "Data = data.copy(deep=True)\n",
    "\n",
    "# ##one hot encode these features\n",
    "encode_list = ['sex', 'age', 'generation']\n",
    "for feature in encode_list:\n",
    "    data = pd.concat((data, pd.get_dummies(data[feature], drop_first=False)),1)\n",
    "    data = data.drop(feature, axis = 1)\n",
    "\n",
    "#drop dependent variable and country\n",
    "# Note country only made about a 1 person per 100k difference in MAE so decided to drop\n",
    "y = data.suicides_100k_pop.values\n",
    "data.drop(['suicides_100k_pop', 'country', 'year'], axis=1, inplace=True)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "##sanity check\n",
    "features_e = np.array(list(data))\n",
    "features_el = list(data)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_scaler = StandardScaler().fit(data.values)\n",
    "X_scaled = x_scaler.transform(data.values)\n",
    "\n",
    "##importing train_test_split from sklearn for data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "##extract data of interest from X_test and Y_test\n",
    "##this includes all Male, Generation X, and age samples within the test set which will be used \n",
    "##for prediction\n",
    "X_test_20 = x_scaler.inverse_transform(X_test)\n",
    "test_df = pd.DataFrame(X_test_20, columns = features_e)\n",
    "test_df['y_test'] = y_test\n",
    "test_df = test_df[(test_df['15-24 years'] == 1) & (test_df['Generation X'] == 1) &\n",
    "                        (test_df['male'] == 1)] \n",
    "y_test_20 = test_df['y_test'].values\n",
    "test_df.drop('y_test', axis=1, inplace=True)\n",
    "test_df.to_csv('test_df_encoded.csv')\n",
    "X_test_20 = x_scaler.transform(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6f0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Preprocess Summary\n",
      "------------------------\n",
      "Pre-encoded Features:  ['country', 'year', 'sex', 'age', 'suicides_no', 'population', 'suicides_100k_pop', 'gdp_per_capita_dollars', 'generation']\n",
      "Dependant Variable: Suicides_100k_pop\n",
      "Number of Encoded features:  15\n",
      "X_test shape:  (129, 15)\n",
      "y_test shape:  (129, 1)\n"
     ]
    }
   ],
   "source": [
    "##sanity check and preprocess summary\n",
    "print('\\n\\nPreprocess Summary')\n",
    "print('------------------------')\n",
    "print('Pre-encoded Features: ', features)\n",
    "print('Dependant Variable: Suicides_100k_pop' )\n",
    "print('Number of Encoded features: ', len(features_el))\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4067fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary: \n",
      "-----------------------\n",
      "R2 Score:  0.31786611617948446\n",
      "Number of Coefficients:  15\n",
      "Intercept:  [21.28316457]\n",
      "Mean-absolue-error (MAE):  15.348952017356728\n",
      "Mean-squared-error (MSE):  568.4010168761149\n",
      "\n",
      "\n",
      "Coef for Each Feature: \n",
      "                 features          coef\n",
      "0             suicides_no  1.073593e+01\n",
      "1              population -6.342996e+00\n",
      "2  gdp_per_capita_dollars -2.455410e-01\n",
      "3                    male  1.397944e+13\n",
      "4             15-24 years  1.525076e+14\n",
      "Wall time: 37.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##train and predict using linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "##get the R2 score which is the percentage of explained variance of the predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "##evaluate performance using mean-absolute-error\n",
    "##Note: adapted form module07_regression_notebook.html\n",
    "def mae(_y: np.array, _y_pred: np.array) -> float:\n",
    "    '''Calculates mean-absolute-error'''\n",
    "    return (len(_y)**-1) * np.sum(np.abs(_y_pred - _y))\n",
    "\n",
    "def mse(_y: np.array, _y_pred: np.array) -> float:\n",
    "    '''Calculates Mean-squared-error'''\n",
    "    return (len(_y)**-1) * np.sum((_y_pred-_y)**2)\n",
    "\n",
    "##calculate metrics\n",
    "MAE = mae(y_test, y_pred)\n",
    "MSE = mse(y_test, y_pred)\n",
    "\n",
    "##Summary: R2-score, intercept, coefficients and MAE\n",
    "print('\\n\\nSummary: ')\n",
    "print('-----------------------')\n",
    "print('R2 Score: ', regressor.score(X_train, y_train))\n",
    "print('Number of Coefficients: ', regressor.coef_.size)\n",
    "print('Intercept: ', regressor.intercept_)\n",
    "print('Mean-absolue-error (MAE): ', MAE)\n",
    "print('Mean-squared-error (MSE): ', MSE)\n",
    "\n",
    "# regressor.coef_.shape\n",
    "print('\\n\\nCoef for Each Feature: ')\n",
    "coeff_df = pd.DataFrame()\n",
    "coeff_df['features'] = features_e.flatten()\n",
    "coeff_df['coef'] = regressor.coef_.flatten()\n",
    "print(coeff_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeac03d",
   "metadata": {},
   "source": [
    "## Now use the original sex, age and generation variables in numerical form and develop a new model. Use your model to predict the target value for the people with age 20, male, and generation X. What is the MAE error of this prediction? How many line coefficients are there? (Note that for this step you have to think of a way of encoding the original nominal age feature and generation feature into numerical features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0a3f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encodings\n",
      "--------------------------------------------------------\n",
      "\n",
      "Unique Ages:  ['55-74 years', '35-54 years', '15-24 years', '5-14 years', '25-34 years', '75+ years']\n",
      "\n",
      "Unique Generations:  ['Millenials', 'Boomers', 'Silent', 'Generation X', 'G.I. Generation']\n",
      "\n",
      "Unique Sex:  ['male']\n",
      "\n",
      "Features:  ['sex', 'age', 'suicides_no', 'population', 'gdp_per_capita_dollars', 'generation']\n",
      "\n",
      "\n",
      "Dataframe Head \n",
      "     sex age  suicides_no  population  gdp_per_capita_dollars generation\n",
      "132   0  30           17      232000                    1299          3\n",
      "133   0  65           10      177400                    1299          1\n",
      "135   0  75            1       24900                    1299          0\n",
      "137   0  20            5      240000                    1299          3\n",
      "140   0  45            4      374700                    1299          2\n",
      "Wall time: 75 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##starting fresh\n",
    "datapath = cwd + '\\\\data\\\\master.csv'\n",
    "\n",
    "##get dataframe\n",
    "data = pd.read_csv(datapath)\n",
    "data = data[data.year == 2000]\n",
    "data = data[data.sex == 'male']\n",
    "\n",
    "def getUnique(col_name: str, dataframe: pd.DataFrame) -> list:\n",
    "    '''Returns unique values from a list'''\n",
    "    values = list(dataframe[col_name])\n",
    "    return list(set(values))\n",
    "\n",
    "print('\\nEncodings')\n",
    "print('--------------------------------------------------------')\n",
    "##encode the 'age' feature with the average age for ease of use\n",
    "ages = getUnique('age', data)\n",
    "print('\\nUnique Ages: ', ages)\n",
    "age_dict = {'5-14 years': 10, '15-24 years': 20, '25-34 years': 30, '35-54 years': 45,\n",
    "           '55-74 years': 65, '75+ years': 75}\n",
    "\n",
    "##encode generations with counting numbers\n",
    "generations = getUnique('generation', data)\n",
    "print('\\nUnique Generations: ', generations)\n",
    "generation_dict = {'G.I. Generation': 0, 'Silent': 1, 'Boomers': 2, 'Generation X': 3,\n",
    "           'Millenials': 4, 'Generation Z': 5}\n",
    "\n",
    "##encode generations with counting numbers\n",
    "sex = getUnique('sex', data)\n",
    "print('\\nUnique Sex: ', sex)\n",
    "sex_dict = {'male': 0, 'female': 1}\n",
    "\n",
    "##change the data values with the encoded values\n",
    "df = data.copy(deep=True)\n",
    "for idx, val in age_dict.items():\n",
    "    df.loc[df.age == idx, 'age'] = val\n",
    "    \n",
    "for idx, val in generation_dict.items():\n",
    "    df.loc[df.generation == idx, 'generation'] = val\n",
    "    \n",
    "for idx, val in sex_dict.items():\n",
    "    df.loc[df.sex == idx, 'sex'] = val\n",
    "\n",
    "##drop dependant variable\n",
    "y = df['suicides_100k_pop'].values\n",
    "df.drop(['suicides_100k_pop', 'country', 'year'], axis=1, inplace=True)\n",
    "\n",
    "##sanity check\n",
    "print('\\nFeatures: ', list(df))\n",
    "print('\\n\\nDataframe Head \\n', df.head())\n",
    "\n",
    "##scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_scaler = StandardScaler().fit(df.values)\n",
    "X_scaled = x_scaler.transform(df.values)\n",
    "\n",
    "##split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "##Extract Male, Generation X, and age samples within the test set which will be used \n",
    "##for prediction\n",
    "X_test_20 = x_scaler.inverse_transform(X_test)\n",
    "test_df = pd.DataFrame(X_test_20, columns = list(df))\n",
    "test_df['y_test'] = y_test\n",
    "test_df = test_df[(test_df['age'] == 20) & (test_df['generation'] == 3) &\n",
    "                        (test_df['sex'] == 0)] \n",
    "y_test_20 = test_df['y_test'].values\n",
    "test_df.drop('y_test', axis=1, inplace=True)\n",
    "X_test_20 = x_scaler.transform(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb57688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary: \n",
      "-----------------------\n",
      "R2 Score:  0.28990148311997077\n",
      "Number of Coefficients:  6\n",
      "Intercept:  21.28646184250393\n",
      "Mean-absolue-error:  16.035183213866983\n",
      "Mean-squared-error (MSE):  568.4702602623679\n",
      "\n",
      "\n",
      "Coef for Each Feature: \n",
      "                 features       coef\n",
      "0                     sex   0.000000\n",
      "1                     age  -1.817471\n",
      "2             suicides_no  10.919402\n",
      "3              population  -6.096847\n",
      "4  gdp_per_capita_dollars  -0.322176\n",
      "5              generation -11.178976\n",
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##train and predict using linear regression\n",
    "regressor = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "##get the R2 score which is the percentage of explained variance of the predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "MAE = mae(y_test, y_pred)\n",
    "MSE = mse(y_test, y_pred)\n",
    "\n",
    "##Summary: R2-score, intercept, coefficients and MAE\n",
    "print('\\n\\nSummary: ')\n",
    "print('-----------------------')\n",
    "print('R2 Score: ', regressor.score(X_train, y_train))\n",
    "print('Number of Coefficients: ', regressor.coef_.size)\n",
    "print('Intercept: ', regressor.intercept_)\n",
    "print('Mean-absolue-error: ', MAE)\n",
    "print('Mean-squared-error (MSE): ', MSE)\n",
    "\n",
    "print('\\n\\nCoef for Each Feature: ')\n",
    "coeff_df = pd.DataFrame()\n",
    "coeff_df['features'] = np.array(list(df)).flatten()\n",
    "coeff_df['coef'] = regressor.coef_.flatten()\n",
    "print(coeff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8157fb",
   "metadata": {},
   "source": [
    "## Any change in these two model performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8daba9",
   "metadata": {},
   "source": [
    "- The MAE of the model trained with one-hot-encoded data is about four percent lower than the model trained on numerical encoded data. The smaller the MAE, the better the performance, so the model trained with numerical encoded data performed better in this case.\n",
    "\n",
    "\n",
    "- The MSE of the model trained with one-hot-encoded data vs the model trained on numerical encoded data is negligable. The smaller the MSE, the better the performance, so the model trained with numerical encoded data performed better in this case.\n",
    "\n",
    "\n",
    "- The R2 score of the model trained with one-hot-encoded data is about nine percent higher than the model trained on numerical encoded data. R2 is the goodness of fit of the data. It is used to determine how well the model's predictions approximate to real data. The closer the R2 is to 1.0, the better the model fits to the data. In this case, it looks like the one-hot-encoded data has a better R2 score as well.\n",
    "\n",
    "- The wall time for the model trained on one-hot-encoded data was 30% longer than for model trained with the numerical encoded data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c175702",
   "metadata": {},
   "source": [
    "## What is the prediction for age 33, male and generation Alpha (i.e. the generation after generation Z)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085fa0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data: \n",
      " [[       0       33        9   179720     4359        6]\n",
      " [       0       33      485  3237957    11273        6]\n",
      " [       0       33       10   202936     3460        6]\n",
      " [       0       33        0     5568    25974        6]\n",
      " [       0       33      311  1606526    54887        6]\n",
      " [       0       33       85   542986    49181        6]\n",
      " [       0       33        0    28398    30239        6]\n",
      " [       0       33        6   235591    22572        6]\n",
      " [       0       33        0    18826    17034        6]\n",
      " [       0       33      382   723856     6371        6]\n",
      " [       0       33      184   696461    47355        6]\n",
      " [       0       33        4    24053     4923        6]\n",
      " [       0       33     1832 17023785    12161        6]\n",
      " [       0       33       57   566462     7066        6]\n",
      " [       0       33      409  2366854    49974        6]\n",
      " [       0       33      307  1289792    13874        6]\n",
      " [       0       33      436  3779392     6836        6]\n",
      " [       0       33       74   371498     8906        6]\n",
      " [       0       33       44   316471    14232        6]\n",
      " [       0       33      102   710089     5997        6]\n",
      " [       0       33        7    65092    32477        6]\n",
      " [       0       33      168   858416    20862        6]\n",
      " [       0       33       43   329780    61711        6]\n",
      " [       0       33      171  1181648     5206        6]\n",
      " [       0       33       93   400088     3318        6]\n",
      " [       0       33       28    96230    15540        6]\n",
      " [       0       33      119   351262    48939        6]\n",
      " [       0       33      805  3829316    44747        6]\n",
      " [       0       33       16   337200     2775        6]\n",
      " [       0       33      735  4940288    43614        6]\n",
      " [       0       33       53   845373    27886        6]\n",
      " [       0       33        0     9006     8118        6]\n",
      " [       0       33       76  1073765     3260        6]\n",
      " [       0       33       30    53501     3384        6]\n",
      " [       0       33      190   780872    13761        6]\n",
      " [       0       33        8    23610    45179        6]\n",
      " [       0       33       75   391033    52042        6]\n",
      " [       0       33       77   561408    34104        6]\n",
      " [       0       33      341  3839282    36869        6]\n",
      " [       0       33        1   202735     5100        6]\n",
      " [       0       33     2543  7822984    47059        6]\n",
      " [       0       33      808  1310211    10062        6]\n",
      " [       0       33       13   619392    35171        6]\n",
      " [       0       33      122   429972      991        6]\n",
      " [       0       33       59   145331    11951        6]\n",
      " [       0       33      110   198028    12584        6]\n",
      " [       0       33        3    36519   111328        6]\n",
      " [       0       33        0    44119     7806        6]\n",
      " [       0       33        3    31445    22132        6]\n",
      " [       0       33       18   102081     8587        6]\n",
      " [       0       33      936  9704369     9991        6]\n",
      " [       0       33      115  1005583    53302        6]\n",
      " [       0       33       65   272460    36145        6]\n",
      " [       0       33       82   477796     1715        6]\n",
      " [       0       33       62   320142    93638        6]\n",
      " [       0       33        2   490613    21337        6]\n",
      " [       0       33       41   291915     8991        6]\n",
      " [       0       33       36   502955     3619        6]\n",
      " [       0       33      400  7256791     2412        6]\n",
      " [       0       33      806  3151996    13304        6]\n",
      " [       0       33       58   726634    23673        6]\n",
      " [       0       33       39   262714    28267        6]\n",
      " [       0       33       20   466496    74055        6]\n",
      " [       0       33     1301  3936496    22994        6]\n",
      " [       0       33      263  1734890     8190        6]\n",
      " [       0       33     6508 11398127    11307        6]\n",
      " [       0       33        0    13280     8557        6]\n",
      " [       0       33        4     8930     6809        6]\n",
      " [       0       33       77   520649     5682        6]\n",
      " [       0       33        2     8490    11563        6]\n",
      " [       0       33       20   274300    66099        6]\n",
      " [       0       33       76   472324    17387        6]\n",
      " [       0       33       32   161002    24689        6]\n",
      " [       0       33      106  4588964     8128        6]\n",
      " [       0       33      272  3734261    32839        6]\n",
      " [       0       33       19    39368     9176        6]\n",
      " [       0       33      106   593020    55352        6]\n",
      " [       0       33       74   519177    78474        6]\n",
      " [       0       33      688  5293421     5707        6]\n",
      " [       0       33       38   116664    18000        6]\n",
      " [       0       33      248  6363299    11525        6]\n",
      " [       0       33       40   419805     4962        6]\n",
      " [       0       33     1457  3589324     3141        6]\n",
      " [       0       33       39  2271630    36964        6]\n",
      " [       0       33      513  4128369    41798        6]\n",
      " [       0       33     4645 20409866    51989        6]\n",
      " [       0       33       77   236974    12882        6]\n",
      " [       0       33      235  2375259     1533        6]]\n",
      "\n",
      "\n",
      "Prediction Data: \n",
      " [10.28795463 11.73180306 10.29573842 10.61329248 11.51398285 11.40632121\n",
      " 10.75747366 10.85892117 10.41150825  8.28988472 10.83186084 10.08566715\n",
      " 23.78421358 10.61257933 11.86757188  9.99932537 12.89725874 10.20405548\n",
      " 10.48348875 10.46688191 10.81852416 10.56171707 11.70061769 10.67775098\n",
      "  9.96140215 10.28046149 10.8169387  11.01628103 10.45193496 13.3660691\n",
      " 11.62319698 10.17239561 11.20125373  9.89045791 10.08312783 11.06011417\n",
      " 11.30702762 11.122662   14.49623444 10.40742441  3.92624719  5.98506278\n",
      " 11.74955115  9.72353537 10.02683898  9.72692727 12.77504644 10.22230112\n",
      " 10.53607196 10.19508727 18.76923687 12.0329806  10.79348928 10.13580795\n",
      " 12.33332108 11.2787205  10.33573045 10.58767562 18.78572581  9.1088847\n",
      " 11.28324414 10.78557604 12.41546332  6.73538291 10.93582026 22.37279543\n",
      " 10.19039853 10.1079781  10.34489041 10.24192256 11.90062432 10.56607915\n",
      " 10.58415765 16.8635343  14.76720446 10.09885934 11.47720456 12.18653892\n",
      " 13.3692468  10.29667296 18.74433372 10.45304817  4.43760298 14.30471871\n",
      " 13.73767091  8.15293098 10.05873096 12.04268207]\n",
      "\n",
      "Average Suicides per 100k population for generation alpha is:  11.3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I realize that there are quite a few ways to handle this question. For one thing, there\n",
    "are more than just age, sex, and generation. The other independent variables include, suicides_no, and\n",
    "gdp_per_capita_dollars. Another consideration is that generation alpha does not turn 33 until 2043\n",
    "at the ealiest, so year should not be used as was therfore dropped. For these reasons, \n",
    "I created a synthetic dataset based on real data that looks at people in the age range of \n",
    "25-34 years old during 2010 to narrow the data down.  I replaced\n",
    "generation to '6' (numerical encoding for \n",
    "generation alpha) and 33 for all of the age columns. \n",
    "Every other datapoint stayed the same with regards to suicides_no, population, and\n",
    "gdp_per_capita_dollars. This data was saved locally as synth_data.csv but can be seen below as test data.\n",
    "The 89 samples of synthetic test data was then scaled using standard scaler and run through the \n",
    "regression model. It seems straight forward to plot a line through points \n",
    "of data on a plot, and so thought that this would be an alternative approach, though it seems that \n",
    "both methods should be taken with a grain of salt. As can be seen below, the average suicides per 100k \n",
    "population is 11.3 suicides.\n",
    "'''\n",
    "datapath = cwd + '\\\\data\\\\synth_data.csv'\n",
    "X_test_33 = np.array(pd.read_csv(datapath).values)\n",
    "print('\\nTest Data: \\n', X_test_33)\n",
    "\n",
    "X_scaled = x_scaler.transform(X_test_33)\n",
    "y_pred = np.absolute(regressor.predict(X_scaled))\n",
    "y_pred_mean = np.round(np.mean(y_pred), 1)\n",
    "print('\\n\\nPrediction Data: \\n', y_pred)\n",
    "print('\\nAverage Suicides per 100k population for generation alpha is: ', y_pred_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28488502",
   "metadata": {},
   "source": [
    "## List one advantage when using regression (as opposed to classification with nominal features) in terms of input data features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d70fc",
   "metadata": {},
   "source": [
    "An advantage of using regression vs classification with nominal features may not be obvious, but I think a specific example will help. For example, lets take classification with decision trees. The decision tree classifier needs to develop a tree containing all of the unique data values for each independent variable. This is usually fast computation for a moderate sized dataset, but if the independent variables are include continous data, this can be a heavy computation, especially if not using a early stopping. For linear regression on the other hand, it does not matter if there is continous data and will not slow down computation. With regards to the benefits of mapping continous input values to continous output values, regression tends to provide more realistic results that can be used to calculate temporal data, currency data, and other continous features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940998c",
   "metadata": {},
   "source": [
    "## List one advantage when using regular numerical values rather than one-hot encoding for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9af9c",
   "metadata": {},
   "source": [
    "One obvious advantage of using regular numerical values rather than one-hot-encoding is that one hot encoding creates more data. It might not make a difference for small datasets, but large datasets might be a limiting factor. It is possible to have hundreds or thousands of new columns of data that one-hot-encoding produces. The new columns times the number of samples can be catastrophic in terms of clock performance, compute power, and storage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb0e54",
   "metadata": {},
   "source": [
    "## Now that you developed both a classifier and a regression model for the problem in this assignment, which method do you suggest to your machine learning model customer? Classifier or regression? Why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966e108",
   "metadata": {},
   "source": [
    "If the customer wants to predict suicide rates for datapoints that do not exist, then regression is preferable. Datapoints that do not exist would include new age groups, new year, etc.. I think that this would likely be the case for this problem, since we found that the dependent variable is continous and the independent variables are a mix of categorical and continous data. Using classification would require an extra step, such as discretization of the dependent variable to fit the dataset into a discrete datatype. This would add complexity to the data, restrict the model to predicting the discrete values, and most likely be less useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
